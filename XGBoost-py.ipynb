{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with Python\n",
    "\n",
    "### Bagging v. Boosting\n",
    "\n",
    "Random Forest and XGBoost are two common approaches to solving ML problems. Their differences can be understood as follows:\n",
    "\n",
    "**error = bias + variance**\n",
    "\n",
    "Random Forest is a bagging algorithm while XGBoost is a boosting algorithm. \n",
    " \n",
    "- Bagging is an ensemble technique that creates several subsets of training data by sampling with replacement. Each subset is used to train a decision tree and the predictions from each subset are averaged.\n",
    "- Boosting is an ensemble technique that adds new models to correct the error made by the existing models. Models are thus added sequentially.\n",
    "\n",
    "In terms of reducing error, RF reduces variances while boosting reduces bias. Random Forest uses fully grow decision trees (low bias, high variance); Boosting works on shallow trees (high bias, low variance). RF reduces variance by making trees uncorrelated to maximise the decrease in variance. In constrast, boosting reduces bias by aggregating the output from many models.\n",
    "\n",
    "\n",
    "## Load data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_values = pd.read_csv('cleanTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_values.loc[:, train_values.columns != 'status_group']\n",
    "y = train_values.loc[:, train_values.columns == 'status_group']\n",
    "\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status_group is a categorical variable with the values 'functional', 'functional needs repair', and 'non functional'. However, the model cannot recognise these alphabetical values. We have to first convert them to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = []\n",
    "for i in y.values:\n",
    "    if i == 'non functional':\n",
    "        i = 0\n",
    "        num.append(i)\n",
    "    elif i == 'functional needs repair':\n",
    "        i = 1\n",
    "        num.append(i)\n",
    "    else:\n",
    "        i = 2\n",
    "        num.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weich\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "y.status_group = num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define fit XGBoost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgb_randomcv(X, y, param_grid):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "    pipe = make_pipeline(StandardScaler(), \n",
    "                     XGBClassifier(random_state=1))\n",
    "    gs = RandomizedSearchCV(pipe, param_grid, n_iter = 10, cv=3, random_state=1)\n",
    "    fitted_model = gs.fit(X_train, y_train)\n",
    "    print(gs.best_params_)\n",
    "    y_pred = gs.predict(X_test)\n",
    "    print(f1_score(y_test, y_pred, average='micro'))\n",
    "    return fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and fine-tune parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fix paramaters\n",
    "\n",
    "A relatively large learning_rate is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgbclassifier__n_estimators': 1000, 'xgbclassifier__min_child_weight': 5, 'xgbclassifier__max_depth': 9, 'xgbclassifier__learning_rate': 0.1}\n",
      "0.7945005611672277\n"
     ]
    }
   ],
   "source": [
    "param_grid1 = {'xgbclassifier__learning_rate':[0.1],\n",
    "              'xgbclassifier__n_estimators':[1000],\n",
    "              'xgbclassifier__max_depth':range(3,10,2),\n",
    "             'xgbclassifier__min_child_weight':range(1,6,2)}\n",
    "fitted_model1 = fit_xgb_randomcv(X, y.values.ravel(), param_grid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tune max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the optimal min_child_weight = 5 and max_depth=9, we take one value below and above for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgbclassifier__n_estimators': 1000, 'xgbclassifier__min_child_weight': 6, 'xgbclassifier__max_depth': 10, 'xgbclassifier__learning_rate': 0.1}\n",
      "0.7975869809203142\n"
     ]
    }
   ],
   "source": [
    "param_grid2 = {'xgbclassifier__learning_rate':[0.1],\n",
    "              'xgbclassifier__n_estimators':[1000],\n",
    "              'xgbclassifier__max_depth':[8, 9, 10],\n",
    "             'xgbclassifier__min_child_weight':[4, 5, 6]}\n",
    "fitted_model2 = fit_xgb_randomcv(X, y.values.ravel(), param_grid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tune gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgbclassifier__n_estimators': 1000, 'xgbclassifier__min_child_weight': 6, 'xgbclassifier__max_depth': 10, 'xgbclassifier__learning_rate': 0.1, 'xgbclassifier__gamma': 0.2}\n",
      "0.8002244668911336\n"
     ]
    }
   ],
   "source": [
    "param_grid3 = {'xgbclassifier__learning_rate':[0.1],\n",
    "              'xgbclassifier__n_estimators':[1000],\n",
    "              'xgbclassifier__max_depth':[10],\n",
    "               'xgbclassifier__min_child_weight':[6],\n",
    "              'xgbclassifier__gamma':[i/10.0 for i in range(0,5)]}\n",
    "fitted_model3 = fit_xgb_randomcv(X, y.values.ravel(), param_grid3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Reduce learning rate and increase number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('xgbclassifier',\n",
       "                 XGBClassifier(base_score=None, booster=None,\n",
       "                               colsample_bylevel=None, colsample_bynode=None,\n",
       "                               colsample_bytree=None, gamma=0.2, gpu_id=None,\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints=None, learning_rate=0.01,\n",
       "                               max_delta_step=None, max_depth=10,\n",
       "                               min_child_weight=6, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=5000,\n",
       "                               n_jobs=None, num_parallel_tree=None,\n",
       "                               objective='binary:logistic', random_state=1,\n",
       "                               reg_alpha=None, reg_lambda=None,\n",
       "                               scale_pos_weight=None, subsample=None,\n",
       "                               tree_method=None, validate_parameters=False,\n",
       "                               verbosity=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe4 = make_pipeline(StandardScaler(), \n",
    "                 XGBClassifier(random_state=1,\n",
    "                              learning_rate=0.01,\n",
    "                              n_estimators=5000,\n",
    "                              max_depth=10,\n",
    "                              min_child_weight=6,\n",
    "                              gamma=0.2))\n",
    "pipe4.fit(X, y.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8985858585858586"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred4 = pipe4.predict(X)\n",
    "f1_score(y, y_pred4, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = pd.read_csv('cleanTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = pd.get_dummies(test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipe4.predict(test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format = pd.read_csv('dataset/SubmissionFormat.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame(data=predictions,\n",
    "                             columns=submission_format.columns,\n",
    "                             index=submission_format.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = []\n",
    "for i in my_submission.values:\n",
    "    if i == 0:\n",
    "        i = 'non functional'\n",
    "        obj.append(i)\n",
    "    elif i == 1:\n",
    "        i = 'functional needs repair'\n",
    "        obj.append(i)\n",
    "    else:\n",
    "        i = 'functional'\n",
    "        obj.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.status_group = obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
